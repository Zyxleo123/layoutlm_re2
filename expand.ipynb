{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def expand(st_dict, new_path):\n",
    "    st_dict['layoutlmv3.embeddings.position_ids'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_ids'], st_dict['layoutlmv3.embeddings.position_ids'])\n",
    "        , dim=1)\n",
    "    st_dict['layoutlmv3.embeddings.position_embeddings.weight'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_embeddings.weight'], st_dict['layoutlmv3.embeddings.position_embeddings.weight'])\n",
    "        , dim=0)\n",
    "    print(\"expand position_ids to \", st_dict['layoutlmv3.embeddings.position_ids'].shape)\n",
    "    print(\"expand position_embeddings.weight to \", st_dict['layoutlmv3.embeddings.position_embeddings.weight'].shape)\n",
    "    for k in list(st_dict.keys()):\n",
    "        if k.endswith('lam'):\n",
    "            st_dict.pop(k)\n",
    "            print(\"pop \", k)\n",
    "    torch.save(st_dict, new_path)\n",
    "\n",
    "def expand_truncate(st_dict, new_path):\n",
    "    st_dict['layoutlmv3.embeddings.position_ids'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_ids'], st_dict['layoutlmv3.embeddings.position_ids'])\n",
    "        , dim=1)\n",
    "    st_dict['layoutlmv3.embeddings.position_embeddings.weight'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_embeddings.weight'], st_dict['layoutlmv3.embeddings.position_embeddings.weight'])\n",
    "        , dim=0)\n",
    "    print(\"expand position_ids to \", st_dict['layoutlmv3.embeddings.position_ids'].shape)\n",
    "    print(\"expand position_embeddings.weight to \", st_dict['layoutlmv3.embeddings.position_embeddings.weight'].shape)\n",
    "    for k in list(st_dict.keys()):\n",
    "        if k.startswith('classifier.') or k.endswith('lam'):\n",
    "            st_dict.pop(k)\n",
    "            print(\"pop \", k)\n",
    "    torch.save(st_dict, new_path)\n",
    "\n",
    "def expand_three_times_truncate(st_dict, new_path):\n",
    "    st_dict['layoutlmv3.embeddings.position_ids'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_ids'],\n",
    "         st_dict['layoutlmv3.embeddings.position_ids'],\n",
    "         st_dict['layoutlmv3.embeddings.position_ids'])\n",
    "        , dim=1)\n",
    "    st_dict['layoutlmv3.embeddings.position_embeddings.weight'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_embeddings.weight'],\n",
    "         st_dict['layoutlmv3.embeddings.position_embeddings.weight'],\n",
    "         st_dict['layoutlmv3.embeddings.position_embeddings.weight'])\n",
    "        , dim=0)\n",
    "    d = ['x', 'y', 'w', 'h']\n",
    "    for dd in d: \n",
    "        st_dict[f'layoutlmv3.embeddings.{dd}_position_embeddings.weight'] = torch.cat(\n",
    "            (st_dict[f'layoutlmv3.embeddings.{dd}_position_embeddings.weight'], \n",
    "             st_dict[f'layoutlmv3.embeddings.{dd}_position_embeddings.weight'])\n",
    "            , dim=0)\n",
    "    print(\"expand position_ids to \", st_dict['layoutlmv3.embeddings.position_ids'].shape)\n",
    "    print(\"expand position_embeddings.weight to \", st_dict['layoutlmv3.embeddings.position_embeddings.weight'].shape)\n",
    "    for k in list(st_dict.keys()):\n",
    "        if k.startswith('classifier.') or k.endswith('lam'):\n",
    "            st_dict.pop(k)\n",
    "            print(\"pop \", k)\n",
    "    torch.save(st_dict, new_path)\n",
    "\n",
    "def expand_three_times_truncate_g(st_dict, new_path):\n",
    "    st_dict['embeddings.position_ids'] = torch.cat(\n",
    "        (st_dict['embeddings.position_ids'],\n",
    "         st_dict['embeddings.position_ids'],\n",
    "         st_dict['embeddings.position_ids'])\n",
    "        , dim=1)\n",
    "    st_dict['embeddings.position_embeddings.weight'] = torch.cat(\n",
    "        (st_dict['embeddings.position_embeddings.weight'],\n",
    "         st_dict['embeddings.position_embeddings.weight'],\n",
    "         st_dict['embeddings.position_embeddings.weight'])\n",
    "        , dim=0)\n",
    "    # d = ['x', 'y', 'w', 'h']\n",
    "    # for dd in d: \n",
    "    #     st_dict[f'layoutlmv3.embeddings.{dd}_position_embeddings.weight'] = torch.cat(\n",
    "    #         (st_dict[f'embeddings.{dd}_position_embeddings.weight'], \n",
    "    #          st_dict[f'embeddings.{dd}_position_embeddings.weight'])\n",
    "    #         , dim=0)\n",
    "    print(\"expand position_ids to \", st_dict['embeddings.position_ids'].shape)\n",
    "    print(\"expand position_embeddings.weight to \", st_dict['embeddings.position_embeddings.weight'].shape)\n",
    "    for k in list(st_dict.keys()):\n",
    "        if k.startswith('classifier.') or k.endswith('lam'):\n",
    "            st_dict.pop(k)\n",
    "            print(\"pop \", k)\n",
    "    torch.save(st_dict, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layoutlmft.models.layoutlmv3.modeling_layoutlmv3 import LayoutLMv3ForRelationExtraction, LayoutLMv3ForTokenClassification, LayoutLMv3Model\n",
    "pt_514 = LayoutLMv3Model.from_pretrained(\n",
    "    'layoutlmv3-base'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_514_d = pt_514.state_dict()\n",
    "expand_truncate(pt_514_d, \"layoutlmv3-large-ner-1028/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_514_d = pt_514.state_dict()\n",
    "expand_three_times_truncate_g(pt_514_d, \"layoutlmv3-base-1542/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at layoutlmv3-base-1028 and are newly initialized: ['layoutlmv3.encoder.layer.2.attention.self.lam', 'classifier.weight', 'layoutlmv3.encoder.layer.6.attention.self.lam', 'layoutlmv3.encoder.layer.8.attention.self.lam', 'layoutlmv3.encoder.layer.4.attention.self.lam', 'layoutlmv3.encoder.layer.5.attention.self.lam', 'layoutlmv3.encoder.layer.0.attention.self.lam', 'layoutlmv3.encoder.layer.9.attention.self.lam', 'layoutlmv3.encoder.layer.10.attention.self.lam', 'layoutlmv3.encoder.layer.3.attention.self.lam', 'layoutlmv3.encoder.layer.7.attention.self.lam', 'layoutlmv3.encoder.layer.11.attention.self.lam', 'classifier.bias', 'layoutlmv3.encoder.layer.1.attention.self.lam']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at layoutlmv3-base-1542 and are newly initialized: ['encoder.layer.7.attention.self.lam', 'encoder.layer.10.attention.self.lam', 'encoder.layer.4.attention.self.lam', 'encoder.layer.2.attention.self.lam', 'classifier.weight', 'encoder.layer.9.attention.self.lam', 'encoder.layer.0.attention.self.lam', 'encoder.layer.1.attention.self.lam', 'encoder.layer.6.attention.self.lam', 'encoder.layer.11.attention.self.lam', 'encoder.layer.3.attention.self.lam', 'encoder.layer.5.attention.self.lam', 'classifier.bias', 'encoder.layer.8.attention.self.lam']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at layoutlmv3-large-1028 and are newly initialized: ['classifier.weight', 'layoutlmv3.encoder.layer.10.attention.self.lam', 'layoutlmv3.encoder.layer.11.attention.self.lam', 'layoutlmv3.encoder.layer.1.attention.self.lam', 'layoutlmv3.encoder.layer.19.attention.self.lam', 'layoutlmv3.encoder.layer.8.attention.self.lam', 'layoutlmv3.encoder.layer.0.attention.self.lam', 'layoutlmv3.encoder.layer.9.attention.self.lam', 'layoutlmv3.encoder.layer.3.attention.self.lam', 'layoutlmv3.encoder.layer.18.attention.self.lam', 'classifier.bias', 'layoutlmv3.encoder.layer.21.attention.self.lam', 'layoutlmv3.encoder.layer.2.attention.self.lam', 'layoutlmv3.encoder.layer.20.attention.self.lam', 'layoutlmv3.encoder.layer.14.attention.self.lam', 'layoutlmv3.encoder.layer.5.attention.self.lam', 'layoutlmv3.encoder.layer.15.attention.self.lam', 'layoutlmv3.encoder.layer.13.attention.self.lam', 'layoutlmv3.encoder.layer.7.attention.self.lam', 'layoutlmv3.encoder.layer.6.attention.self.lam', 'layoutlmv3.encoder.layer.4.attention.self.lam', 'layoutlmv3.encoder.layer.12.attention.self.lam', 'layoutlmv3.encoder.layer.23.attention.self.lam', 'layoutlmv3.encoder.layer.16.attention.self.lam', 'layoutlmv3.encoder.layer.22.attention.self.lam', 'layoutlmv3.encoder.layer.17.attention.self.lam']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at layoutlmv3-large-1542 and are newly initialized: ['classifier.weight', 'layoutlmv3.encoder.layer.10.attention.self.lam', 'layoutlmv3.encoder.layer.11.attention.self.lam', 'layoutlmv3.encoder.layer.1.attention.self.lam', 'layoutlmv3.encoder.layer.19.attention.self.lam', 'layoutlmv3.encoder.layer.8.attention.self.lam', 'layoutlmv3.encoder.layer.0.attention.self.lam', 'layoutlmv3.encoder.layer.9.attention.self.lam', 'layoutlmv3.encoder.layer.3.attention.self.lam', 'layoutlmv3.encoder.layer.18.attention.self.lam', 'classifier.bias', 'layoutlmv3.encoder.layer.21.attention.self.lam', 'layoutlmv3.encoder.layer.2.attention.self.lam', 'layoutlmv3.encoder.layer.20.attention.self.lam', 'layoutlmv3.encoder.layer.14.attention.self.lam', 'layoutlmv3.encoder.layer.5.attention.self.lam', 'layoutlmv3.encoder.layer.15.attention.self.lam', 'layoutlmv3.encoder.layer.13.attention.self.lam', 'layoutlmv3.encoder.layer.7.attention.self.lam', 'layoutlmv3.encoder.layer.6.attention.self.lam', 'layoutlmv3.encoder.layer.4.attention.self.lam', 'layoutlmv3.encoder.layer.12.attention.self.lam', 'layoutlmv3.encoder.layer.23.attention.self.lam', 'layoutlmv3.encoder.layer.16.attention.self.lam', 'layoutlmv3.encoder.layer.22.attention.self.lam', 'layoutlmv3.encoder.layer.17.attention.self.lam']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import layoutlmft.models.layoutlmv3\n",
    "from layoutlmft.models.layoutlmv3.modeling_layoutlmv3 import LayoutLMv3ForRelationExtraction, LayoutLMv3ForTokenClassification, LayoutLMv3Model\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config_1028 = AutoConfig.from_pretrained(\n",
    "    'layoutlmv3-base-1028'\n",
    ")\n",
    "pt_base_1028 = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    'layoutlmv3-base-1028',\n",
    "    config=config_1028\n",
    ")\n",
    "config_1542 = AutoConfig.from_pretrained(\n",
    "    'layoutlmv3-base-1542'\n",
    ")\n",
    "pt_base_1542 = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    'layoutlmv3-base-1542',\n",
    "    config=config_1542\n",
    ")\n",
    "config_large_1028 = AutoConfig.from_pretrained(\n",
    "    'layoutlmv3-large-1028'\n",
    ")\n",
    "pt_large_1028 = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    'layoutlmv3-large-1028',\n",
    "    config=config_large_1028\n",
    ")\n",
    "config_large_1542 = AutoConfig.from_pretrained(\n",
    "    'layoutlmv3-large-1542'\n",
    ")\n",
    "pt_large_1542 = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    'layoutlmv3-large-1542',\n",
    "    config=config_large_1542\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layoutlmv3.cls_token torch.Size([1, 1, 768]) layoutlmv3.cls_token torch.Size([1, 1, 768])\n",
      "layoutlmv3.pos_embed torch.Size([1, 197, 768]) layoutlmv3.pos_embed torch.Size([1, 197, 768])\n",
      "layoutlmv3.embeddings.position_ids torch.Size([1, 1028]) layoutlmv3.embeddings.position_ids torch.Size([1, 1542])\n",
      "layoutlmv3.embeddings.word_embeddings.weight torch.Size([50265, 768]) layoutlmv3.embeddings.word_embeddings.weight torch.Size([50265, 768])\n",
      "layoutlmv3.embeddings.token_type_embeddings.weight torch.Size([1, 768]) layoutlmv3.embeddings.token_type_embeddings.weight torch.Size([1, 768])\n",
      "layoutlmv3.embeddings.LayerNorm.weight torch.Size([768]) layoutlmv3.embeddings.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.embeddings.LayerNorm.bias torch.Size([768]) layoutlmv3.embeddings.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.embeddings.position_embeddings.weight torch.Size([1028, 768]) layoutlmv3.embeddings.position_embeddings.weight torch.Size([1542, 768])\n",
      "layoutlmv3.embeddings.x_position_embeddings.weight torch.Size([1024, 128]) layoutlmv3.embeddings.x_position_embeddings.weight torch.Size([1024, 128])\n",
      "layoutlmv3.embeddings.y_position_embeddings.weight torch.Size([1024, 128]) layoutlmv3.embeddings.y_position_embeddings.weight torch.Size([1024, 128])\n",
      "layoutlmv3.embeddings.h_position_embeddings.weight torch.Size([1024, 128]) layoutlmv3.embeddings.h_position_embeddings.weight torch.Size([1024, 128])\n",
      "layoutlmv3.embeddings.w_position_embeddings.weight torch.Size([1024, 128]) layoutlmv3.embeddings.w_position_embeddings.weight torch.Size([1024, 128])\n",
      "layoutlmv3.encoder.layer.0.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.0.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.0.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.0.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.0.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.0.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.0.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.0.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.0.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.0.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.0.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.0.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.0.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.0.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.1.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.1.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.1.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.1.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.1.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.1.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.1.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.1.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.1.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.1.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.1.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.1.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.1.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.1.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.2.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.2.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.2.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.2.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.2.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.2.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.2.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.2.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.2.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.2.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.2.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.2.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.2.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.2.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.3.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.3.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.3.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.3.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.3.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.3.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.3.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.3.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.3.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.3.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.3.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.3.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.3.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.3.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.4.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.4.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.4.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.4.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.4.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.4.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.4.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.4.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.4.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.4.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.4.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.4.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.4.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.4.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.5.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.5.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.5.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.5.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.5.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.5.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.5.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.5.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.5.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.5.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.5.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.5.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.5.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.5.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.6.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.6.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.6.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.6.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.6.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.6.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.6.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.6.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.6.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.6.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.6.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.6.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.6.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.6.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.7.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.7.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.7.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.7.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.7.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.7.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.7.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.7.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.7.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.7.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.7.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.7.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.7.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.7.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.8.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.8.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.8.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.8.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.8.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.8.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.8.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.8.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.8.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.8.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.8.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.8.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.8.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.8.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.9.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.9.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.9.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.9.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.9.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.9.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.9.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.9.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.9.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.9.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.9.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.9.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.9.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.9.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.10.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.10.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.10.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.10.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.10.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.10.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.10.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.10.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.10.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.10.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.10.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.10.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.10.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.10.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.11.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.11.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.11.attention.self.query.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.11.attention.self.query.bias torch.Size([768]) layoutlmv3.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.11.attention.self.key.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.11.attention.self.key.bias torch.Size([768]) layoutlmv3.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.11.attention.self.value.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.11.attention.self.value.bias torch.Size([768]) layoutlmv3.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768]) layoutlmv3.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "layoutlmv3.encoder.layer.11.attention.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768]) layoutlmv3.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "layoutlmv3.encoder.layer.11.intermediate.dense.bias torch.Size([3072]) layoutlmv3.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "layoutlmv3.encoder.layer.11.output.dense.weight torch.Size([768, 3072]) layoutlmv3.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "layoutlmv3.encoder.layer.11.output.dense.bias torch.Size([768]) layoutlmv3.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "layoutlmv3.encoder.layer.11.output.LayerNorm.weight torch.Size([768]) layoutlmv3.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.encoder.layer.11.output.LayerNorm.bias torch.Size([768]) layoutlmv3.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.encoder.rel_pos_bias.weight torch.Size([12, 32]) layoutlmv3.encoder.rel_pos_bias.weight torch.Size([12, 32])\n",
      "layoutlmv3.encoder.rel_pos_x_bias.weight torch.Size([12, 64]) layoutlmv3.encoder.rel_pos_x_bias.weight torch.Size([12, 64])\n",
      "layoutlmv3.encoder.rel_pos_y_bias.weight torch.Size([12, 64]) layoutlmv3.encoder.rel_pos_y_bias.weight torch.Size([12, 64])\n",
      "layoutlmv3.patch_embed.proj.weight torch.Size([768, 3, 16, 16]) layoutlmv3.patch_embed.proj.weight torch.Size([768, 3, 16, 16])\n",
      "layoutlmv3.patch_embed.proj.bias torch.Size([768]) layoutlmv3.patch_embed.proj.bias torch.Size([768])\n",
      "layoutlmv3.LayerNorm.weight torch.Size([768]) layoutlmv3.LayerNorm.weight torch.Size([768])\n",
      "layoutlmv3.LayerNorm.bias torch.Size([768]) layoutlmv3.LayerNorm.bias torch.Size([768])\n",
      "layoutlmv3.norm.weight torch.Size([768]) layoutlmv3.norm.weight torch.Size([768])\n",
      "layoutlmv3.norm.bias torch.Size([768]) layoutlmv3.norm.bias torch.Size([768])\n",
      "classifier.weight torch.Size([2, 768]) classifier.weight torch.Size([2, 768])\n",
      "classifier.bias torch.Size([2]) classifier.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "assert len(pt_base_1028.state_dict()) == len(pt_base_1542.state_dict())\n",
    "for (k1, v1),(k2, v2) in zip(pt_base_1028.state_dict().items(), pt_base_1542.state_dict().items()):\n",
    "    print(k1, v1.shape, k2, v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layoutlmv3.cls_token torch.Size([1, 1, 1024]) layoutlmv3.cls_token torch.Size([1, 1, 1024])\n",
      "layoutlmv3.pos_embed torch.Size([1, 197, 1024]) layoutlmv3.pos_embed torch.Size([1, 197, 1024])\n",
      "layoutlmv3.embeddings.position_ids torch.Size([1, 1028]) layoutlmv3.embeddings.position_ids torch.Size([1, 1542])\n",
      "layoutlmv3.embeddings.word_embeddings.weight torch.Size([50265, 1024]) layoutlmv3.embeddings.word_embeddings.weight torch.Size([50265, 1024])\n",
      "layoutlmv3.embeddings.token_type_embeddings.weight torch.Size([1, 1024]) layoutlmv3.embeddings.token_type_embeddings.weight torch.Size([1, 1024])\n",
      "layoutlmv3.embeddings.LayerNorm.weight torch.Size([1024]) layoutlmv3.embeddings.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.embeddings.LayerNorm.bias torch.Size([1024]) layoutlmv3.embeddings.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.embeddings.position_embeddings.weight torch.Size([1028, 1024]) layoutlmv3.embeddings.position_embeddings.weight torch.Size([1542, 1024])\n",
      "layoutlmv3.embeddings.x_position_embeddings.weight torch.Size([1024, 171]) layoutlmv3.embeddings.x_position_embeddings.weight torch.Size([2048, 171])\n",
      "layoutlmv3.embeddings.y_position_embeddings.weight torch.Size([1024, 171]) layoutlmv3.embeddings.y_position_embeddings.weight torch.Size([2048, 171])\n",
      "layoutlmv3.embeddings.h_position_embeddings.weight torch.Size([1024, 170]) layoutlmv3.embeddings.h_position_embeddings.weight torch.Size([2048, 170])\n",
      "layoutlmv3.embeddings.w_position_embeddings.weight torch.Size([1024, 170]) layoutlmv3.embeddings.w_position_embeddings.weight torch.Size([2048, 170])\n",
      "layoutlmv3.encoder.layer.0.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.0.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.0.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.0.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.0.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.0.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.0.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.0.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.0.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.0.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.0.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.0.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.0.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.0.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.0.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.0.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.0.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.0.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.0.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.0.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.0.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.0.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.0.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.1.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.1.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.1.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.1.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.1.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.1.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.1.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.1.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.1.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.1.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.1.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.1.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.1.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.1.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.1.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.1.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.1.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.1.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.1.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.1.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.1.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.1.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.1.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.2.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.2.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.2.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.2.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.2.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.2.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.2.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.2.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.2.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.2.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.2.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.2.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.2.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.2.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.2.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.2.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.2.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.2.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.2.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.2.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.2.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.2.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.2.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.3.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.3.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.3.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.3.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.3.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.3.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.3.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.3.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.3.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.3.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.3.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.3.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.3.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.3.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.3.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.3.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.3.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.3.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.3.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.3.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.3.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.3.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.3.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.4.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.4.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.4.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.4.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.4.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.4.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.4.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.4.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.4.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.4.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.4.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.4.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.4.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.4.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.4.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.4.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.4.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.4.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.4.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.4.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.4.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.4.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.4.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.5.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.5.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.5.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.5.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.5.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.5.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.5.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.5.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.5.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.5.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.5.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.5.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.5.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.5.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.5.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.5.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.5.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.5.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.5.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.5.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.5.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.5.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.5.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.6.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.6.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.6.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.6.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.6.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.6.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.6.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.6.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.6.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.6.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.6.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.6.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.6.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.6.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.6.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.6.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.6.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.6.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.6.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.6.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.6.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.6.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.6.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.7.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.7.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.7.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.7.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.7.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.7.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.7.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.7.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.7.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.7.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.7.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.7.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.7.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.7.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.7.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.7.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.7.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.7.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.7.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.7.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.7.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.7.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.7.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.8.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.8.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.8.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.8.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.8.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.8.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.8.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.8.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.8.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.8.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.8.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.8.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.8.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.8.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.8.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.8.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.8.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.8.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.8.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.8.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.8.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.8.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.8.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.9.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.9.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.9.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.9.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.9.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.9.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.9.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.9.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.9.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.9.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.9.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.9.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.9.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.9.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.9.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.9.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.9.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.9.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.9.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.9.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.9.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.9.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.9.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.10.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.10.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.10.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.10.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.10.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.10.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.10.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.10.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.10.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.10.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.10.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.10.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.10.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.10.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.10.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.10.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.10.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.10.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.10.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.10.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.10.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.10.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.10.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.11.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.11.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.11.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.11.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.11.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.11.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.11.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.11.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.11.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.11.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.11.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.11.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.11.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.11.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.11.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.11.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.11.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.11.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.11.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.11.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.11.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.11.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.11.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.12.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.12.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.12.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.12.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.12.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.12.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.12.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.12.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.12.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.12.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.12.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.12.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.12.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.12.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.12.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.12.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.12.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.12.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.12.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.12.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.12.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.12.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.12.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.13.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.13.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.13.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.13.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.13.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.13.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.13.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.13.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.13.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.13.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.13.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.13.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.13.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.13.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.13.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.13.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.13.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.13.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.13.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.13.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.13.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.13.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.13.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.14.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.14.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.14.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.14.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.14.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.14.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.14.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.14.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.14.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.14.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.14.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.14.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.14.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.14.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.14.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.14.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.14.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.14.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.14.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.14.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.14.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.14.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.14.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.15.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.15.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.15.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.15.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.15.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.15.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.15.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.15.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.15.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.15.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.15.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.15.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.15.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.15.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.15.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.15.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.15.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.15.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.15.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.15.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.15.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.15.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.15.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.16.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.16.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.16.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.16.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.16.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.16.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.16.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.16.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.16.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.16.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.16.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.16.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.16.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.16.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.16.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.16.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.16.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.16.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.16.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.16.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.16.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.16.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.16.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.17.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.17.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.17.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.17.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.17.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.17.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.17.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.17.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.17.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.17.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.17.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.17.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.17.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.17.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.17.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.17.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.17.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.17.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.17.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.17.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.17.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.17.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.17.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.18.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.18.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.18.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.18.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.18.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.18.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.18.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.18.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.18.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.18.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.18.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.18.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.18.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.18.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.18.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.18.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.18.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.18.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.18.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.18.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.18.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.18.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.18.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.19.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.19.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.19.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.19.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.19.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.19.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.19.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.19.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.19.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.19.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.19.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.19.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.19.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.19.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.19.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.19.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.19.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.19.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.19.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.19.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.19.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.19.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.19.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.20.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.20.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.20.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.20.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.20.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.20.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.20.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.20.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.20.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.20.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.20.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.20.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.20.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.20.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.20.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.20.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.20.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.20.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.20.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.20.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.20.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.20.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.20.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.21.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.21.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.21.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.21.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.21.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.21.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.21.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.21.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.21.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.21.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.21.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.21.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.21.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.21.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.21.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.21.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.21.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.21.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.21.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.21.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.21.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.21.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.21.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.22.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.22.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.22.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.22.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.22.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.22.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.22.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.22.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.22.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.22.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.22.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.22.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.22.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.22.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.22.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.22.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.22.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.22.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.22.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.22.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.22.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.22.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.22.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.23.attention.self.lam torch.Size([]) layoutlmv3.encoder.layer.23.attention.self.lam torch.Size([])\n",
      "layoutlmv3.encoder.layer.23.attention.self.query.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.23.attention.self.query.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.23.attention.self.query.bias torch.Size([1024]) layoutlmv3.encoder.layer.23.attention.self.query.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.23.attention.self.key.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.23.attention.self.key.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.23.attention.self.key.bias torch.Size([1024]) layoutlmv3.encoder.layer.23.attention.self.key.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.23.attention.self.value.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.23.attention.self.value.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.23.attention.self.value.bias torch.Size([1024]) layoutlmv3.encoder.layer.23.attention.self.value.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024]) layoutlmv3.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "layoutlmv3.encoder.layer.23.attention.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024]) layoutlmv3.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "layoutlmv3.encoder.layer.23.intermediate.dense.bias torch.Size([4096]) layoutlmv3.encoder.layer.23.intermediate.dense.bias torch.Size([4096])\n",
      "layoutlmv3.encoder.layer.23.output.dense.weight torch.Size([1024, 4096]) layoutlmv3.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "layoutlmv3.encoder.layer.23.output.dense.bias torch.Size([1024]) layoutlmv3.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.23.output.LayerNorm.weight torch.Size([1024]) layoutlmv3.encoder.layer.23.output.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.encoder.layer.23.output.LayerNorm.bias torch.Size([1024]) layoutlmv3.encoder.layer.23.output.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.encoder.rel_pos_bias.weight torch.Size([16, 32]) layoutlmv3.encoder.rel_pos_bias.weight torch.Size([16, 32])\n",
      "layoutlmv3.encoder.rel_pos_x_bias.weight torch.Size([16, 64]) layoutlmv3.encoder.rel_pos_x_bias.weight torch.Size([16, 64])\n",
      "layoutlmv3.encoder.rel_pos_y_bias.weight torch.Size([16, 64]) layoutlmv3.encoder.rel_pos_y_bias.weight torch.Size([16, 64])\n",
      "layoutlmv3.patch_embed.proj.weight torch.Size([1024, 3, 16, 16]) layoutlmv3.patch_embed.proj.weight torch.Size([1024, 3, 16, 16])\n",
      "layoutlmv3.patch_embed.proj.bias torch.Size([1024]) layoutlmv3.patch_embed.proj.bias torch.Size([1024])\n",
      "layoutlmv3.LayerNorm.weight torch.Size([1024]) layoutlmv3.LayerNorm.weight torch.Size([1024])\n",
      "layoutlmv3.LayerNorm.bias torch.Size([1024]) layoutlmv3.LayerNorm.bias torch.Size([1024])\n",
      "layoutlmv3.norm.weight torch.Size([1024]) layoutlmv3.norm.weight torch.Size([1024])\n",
      "layoutlmv3.norm.bias torch.Size([1024]) layoutlmv3.norm.bias torch.Size([1024])\n",
      "classifier.weight torch.Size([2, 1024]) classifier.weight torch.Size([2, 1024])\n",
      "classifier.bias torch.Size([2]) classifier.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "assert len(pt_large_1028.state_dict()) == len(pt_large_1542.state_dict())\n",
    "for (k1, v1),(k2, v2) in zip(pt_large_1028.state_dict().items(), pt_large_1542.state_dict().items()):\n",
    "    print(k1, v1.shape, k2, v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
