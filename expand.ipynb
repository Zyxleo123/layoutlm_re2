{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/mambaforge/envs/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def expand(st_dict, new_path):\n",
    "    st_dict['layoutlmv3.embeddings.position_ids'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_ids'], st_dict['layoutlmv3.embeddings.position_ids'])\n",
    "        , dim=1)\n",
    "    st_dict['layoutlmv3.embeddings.position_embeddings.weight'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_embeddings.weight'], st_dict['layoutlmv3.embeddings.position_embeddings.weight'])\n",
    "        , dim=0)\n",
    "    print(\"expand position_ids to \", st_dict['layoutlmv3.embeddings.position_ids'].shape)\n",
    "    print(\"expand position_embeddings.weight to \", st_dict['layoutlmv3.embeddings.position_embeddings.weight'].shape)\n",
    "    for k in list(st_dict.keys()):\n",
    "        if k.endswith('lam'):\n",
    "            st_dict.pop(k)\n",
    "            print(\"pop \", k)\n",
    "    torch.save(st_dict, new_path)\n",
    "\n",
    "def expand_truncate(st_dict, new_path):\n",
    "    st_dict['layoutlmv3.embeddings.position_ids'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_ids'], st_dict['layoutlmv3.embeddings.position_ids'])\n",
    "        , dim=1)\n",
    "    st_dict['layoutlmv3.embeddings.position_embeddings.weight'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_embeddings.weight'], st_dict['layoutlmv3.embeddings.position_embeddings.weight'])\n",
    "        , dim=0)\n",
    "    print(\"expand position_ids to \", st_dict['layoutlmv3.embeddings.position_ids'].shape)\n",
    "    print(\"expand position_embeddings.weight to \", st_dict['layoutlmv3.embeddings.position_embeddings.weight'].shape)\n",
    "    for k in list(st_dict.keys()):\n",
    "        if k.startswith('classifier.') or k.endswith('lam'):\n",
    "            st_dict.pop(k)\n",
    "            print(\"pop \", k)\n",
    "    torch.save(st_dict, new_path)\n",
    "\n",
    "def expand_three_times_truncate(st_dict, new_path):\n",
    "    st_dict['layoutlmv3.embeddings.position_ids'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_ids'],\n",
    "         st_dict['layoutlmv3.embeddings.position_ids'],\n",
    "         st_dict['layoutlmv3.embeddings.position_ids'])\n",
    "        , dim=1)\n",
    "    st_dict['layoutlmv3.embeddings.position_embeddings.weight'] = torch.cat(\n",
    "        (st_dict['layoutlmv3.embeddings.position_embeddings.weight'],\n",
    "         st_dict['layoutlmv3.embeddings.position_embeddings.weight'],\n",
    "         st_dict['layoutlmv3.embeddings.position_embeddings.weight'])\n",
    "        , dim=0)\n",
    "    d = ['x', 'y', 'w', 'h']\n",
    "    for dd in d: \n",
    "        st_dict[f'layoutlmv3.embeddings.{dd}_position_embeddings.weight'] = torch.cat(\n",
    "            (st_dict[f'layoutlmv3.embeddings.{dd}_position_embeddings.weight'], \n",
    "             st_dict[f'layoutlmv3.embeddings.{dd}_position_embeddings.weight'])\n",
    "            , dim=0)\n",
    "    print(\"expand position_ids to \", st_dict['layoutlmv3.embeddings.position_ids'].shape)\n",
    "    print(\"expand position_embeddings.weight to \", st_dict['layoutlmv3.embeddings.position_embeddings.weight'].shape)\n",
    "    print(\"expand x_position_embeddings.weight to \", st_dict['layoutlmv3.embeddings.x_position_embeddings.weight'].shape)\n",
    "    print(\"expand y_position_embeddings.weight to \", st_dict['layoutlmv3.embeddings.y_position_embeddings.weight'].shape)\n",
    "    print(\"expand w_position_embeddings.weight to \", st_dict['layoutlmv3.embeddings.w_position_embeddings.weight'].shape)\n",
    "    print(\"expand h_position_embeddings.weight to \", st_dict['layoutlmv3.embeddings.h_position_embeddings.weight'].shape)\n",
    "    for k in list(st_dict.keys()):\n",
    "        if k.startswith('classifier.') or k.endswith('lam'):\n",
    "            st_dict.pop(k)\n",
    "            print(\"pop \", k)\n",
    "    torch.save(st_dict, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layoutlmft.models.layoutlmv3.modeling_layoutlmv3 import LayoutLMv3ForRelationExtraction, LayoutLMv3ForTokenClassification\n",
    "# model_ft = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "#     \"HYPJUDY/layoutlmv3-large-finetuned-funsd\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-large and are newly initialized: ['layoutlmv3.encoder.layer.11.attention.self.lam', 'layoutlmv3.encoder.layer.9.attention.self.lam', 'layoutlmv3.encoder.layer.20.attention.self.lam', 'layoutlmv3.encoder.layer.23.attention.self.lam', 'layoutlmv3.encoder.layer.22.attention.self.lam', 'layoutlmv3.encoder.layer.21.attention.self.lam', 'layoutlmv3.encoder.layer.0.attention.self.lam', 'layoutlmv3.encoder.layer.7.attention.self.lam', 'layoutlmv3.encoder.layer.13.attention.self.lam', 'classifier.weight', 'layoutlmv3.encoder.layer.14.attention.self.lam', 'layoutlmv3.encoder.layer.4.attention.self.lam', 'layoutlmv3.encoder.layer.6.attention.self.lam', 'layoutlmv3.encoder.layer.5.attention.self.lam', 'layoutlmv3.encoder.layer.17.attention.self.lam', 'layoutlmv3.encoder.layer.15.attention.self.lam', 'layoutlmv3.encoder.layer.3.attention.self.lam', 'layoutlmv3.encoder.layer.18.attention.self.lam', 'layoutlmv3.encoder.layer.1.attention.self.lam', 'layoutlmv3.encoder.layer.8.attention.self.lam', 'layoutlmv3.encoder.layer.19.attention.self.lam', 'layoutlmv3.encoder.layer.16.attention.self.lam', 'layoutlmv3.encoder.layer.10.attention.self.lam', 'layoutlmv3.encoder.layer.2.attention.self.lam', 'classifier.bias', 'layoutlmv3.encoder.layer.12.attention.self.lam']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# re_pretrain = LayoutLMv3ForRelationExtraction.from_pretrained(\n",
    "#     \"microsoft/layoutlmv3-large\"\n",
    "# )\n",
    "ner_pretrain = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re_st_dict = re_pretrain.state_dict()\n",
    "# expand(re_st_dict, \"layoutlmv3-large-re-1028/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expand position_ids to  torch.Size([1, 1028])\n",
      "expand position_embeddings.weight to  torch.Size([1028, 1024])\n",
      "pop  layoutlmv3.encoder.layer.0.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.1.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.2.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.3.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.4.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.5.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.6.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.7.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.8.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.9.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.10.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.11.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.12.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.13.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.14.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.15.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.16.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.17.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.18.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.19.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.20.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.21.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.22.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.23.attention.self.lam\n",
      "pop  classifier.weight\n",
      "pop  classifier.bias\n"
     ]
    }
   ],
   "source": [
    "ner_st_dict = ner_pretrain.state_dict()\n",
    "expand_truncate(ner_st_dict, \"layoutlmv3-large-ner-1028/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expand position_ids to  torch.Size([1, 1542])\n",
      "expand position_embeddings.weight to  torch.Size([1542, 1024])\n",
      "expand x_position_embeddings.weight to  torch.Size([2048, 171])\n",
      "expand y_position_embeddings.weight to  torch.Size([2048, 171])\n",
      "expand w_position_embeddings.weight to  torch.Size([2048, 170])\n",
      "expand h_position_embeddings.weight to  torch.Size([2048, 170])\n",
      "pop  layoutlmv3.encoder.layer.0.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.1.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.2.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.3.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.4.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.5.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.6.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.7.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.8.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.9.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.10.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.11.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.12.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.13.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.14.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.15.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.16.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.17.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.18.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.19.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.20.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.21.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.22.attention.self.lam\n",
      "pop  layoutlmv3.encoder.layer.23.attention.self.lam\n",
      "pop  classifier.weight\n",
      "pop  classifier.bias\n"
     ]
    }
   ],
   "source": [
    "ner_st_dict = ner_pretrain.state_dict()\n",
    "expand_three_times_truncate(ner_st_dict, \"layoutlmv3-large-ner-1542/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re_ft_st_dict = model_ft.state_dict()\n",
    "# expand_truncate(re_ft_st_dict, \"layoutlmv3-large-re-1028-ft/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_ft_st_dict = model_ft.state_dict()\n",
    "# expand(ner_ft_st_dict, \"layoutlmv3-large-ner-1028-ft/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at layoutlmv3-large-ner-1028 and are newly initialized: ['layoutlmv3.encoder.layer.6.attention.self.lam', 'layoutlmv3.encoder.layer.9.attention.self.lam', 'layoutlmv3.encoder.layer.4.attention.self.lam', 'layoutlmv3.encoder.layer.13.attention.self.lam', 'layoutlmv3.encoder.layer.14.attention.self.lam', 'layoutlmv3.encoder.layer.10.attention.self.lam', 'layoutlmv3.encoder.layer.21.attention.self.lam', 'layoutlmv3.encoder.layer.5.attention.self.lam', 'layoutlmv3.encoder.layer.7.attention.self.lam', 'classifier.weight', 'layoutlmv3.encoder.layer.12.attention.self.lam', 'layoutlmv3.encoder.layer.2.attention.self.lam', 'layoutlmv3.encoder.layer.23.attention.self.lam', 'layoutlmv3.encoder.layer.3.attention.self.lam', 'layoutlmv3.encoder.layer.15.attention.self.lam', 'layoutlmv3.encoder.layer.19.attention.self.lam', 'layoutlmv3.encoder.layer.0.attention.self.lam', 'layoutlmv3.encoder.layer.8.attention.self.lam', 'layoutlmv3.encoder.layer.11.attention.self.lam', 'layoutlmv3.encoder.layer.18.attention.self.lam', 'layoutlmv3.encoder.layer.16.attention.self.lam', 'layoutlmv3.encoder.layer.22.attention.self.lam', 'classifier.bias', 'layoutlmv3.encoder.layer.20.attention.self.lam', 'layoutlmv3.encoder.layer.17.attention.self.lam', 'layoutlmv3.encoder.layer.1.attention.self.lam']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356533786\n"
     ]
    }
   ],
   "source": [
    "from layoutlmft.models.layoutlmv3.modeling_layoutlmv3 import LayoutLMv3ForTokenClassification\n",
    "large_ner_1028 = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    \"layoutlmv3-large-ner-1028\"\n",
    ")\n",
    "print(count_parameters(large_ner_1028))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_path = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    \"layoutlmv3-large-ner-1028\",\n",
    "    # num_labels=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
